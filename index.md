## Track Overview
Search engine results underpin many consequential decision making tasks. Examples include
people using search technologies to seek health advice online, or time-pressured clinicians
relying on search results to decide upon the best treatment/diagnosis/test for a patient.

A key problem when using search engines in order to complete such decision making tasks, is
whether users are able to discern authoritative from unreliable information and correct from
incorrect information. This problem is further exacerbated when the search occurs within
uncontrolled data collections, such as the web, where information can be unreliable, generally
misleading, too technical, and can lead to unfounded escalations (White&Horvitz, 2009).
Information from search engine results can significantly influence decisions, and research
shows that increasing the amount of incorrect information about a topic presented in a SERP
can impel users to take incorrect decisions (Pogacar et al, 2017). As noted in the SWIRL III
report (Culpepper et al., 2018), decision making with search engines is poorly understood, and
likewise, evaluation measures for these search tasks need to be developed and improved.

## Goals of the Track

This track aims to 
* Provide a venue for research on retrieval methods that promote better
decision making with search engines, and
* Develop new online and offline evaluation
methods to predict the decision making quality induced by search results.

## Task
The track is planned over multiple years, with data and resources created in one year flowing
into the next year. We plan for the track to run for at least 3 years.

#### Year 1 (2019)
Participants devise search technologies that promote correct information over incorrect information, with the assumption that correct information can better lead people to make correct decisions.

Note: this task is more than simply a new definition of what is relevant. There are 3 types of results: correct and relevant, incorrect, and non-relevant. It is important that search results avoid containing incorrect results, and
ranking non-relevant results above incorrect is preferred.

Evaluation measures will consider relevance beyond topicality, indlucuding correctness of information and crediablity. 

Following the year 1 assessment, the organizers will recruit test subjects to perform a decision
making task using a selection of the year 1 runs. That is, test subjects will be given a fixed result list (selected from submitted runs) and a decision task. We will collect user interaction data as well as the users' decisions.

#### Year 2 (2020)
Given a query, a document ranking (results list) and interaction data (collected during year 1),
predict the decisions users will take at the end of the search process, along with their confidence when taking such decisions. This simulates an online evaluation process.

#### Year 3 (2021)
Given a query, a document ranking (results list), and assessments, predict the decision the user will take at the end of the search process (along with the confidence expressed by the user with respect to their decision). 
This simulates an offline evaluation process.


## Data

#### Search Topics
TBA

#### Collection (documents)
TBA

## Submission of Runs
TBA

#### Format
TBA

## Evaluation of Runs
TBA

## Schedule  
#### 2019
* (TBA) Final guidelines available
* (TBA) Availability of Topics/Collection
* (TBA) Runs due
* (TBA) Results returned
* (TBA) Notebook paper due
* (TBA) TREC Conference
* (TBA) Final report due


## Organizers

#### Christina Lioma, University of Copenhagen        
#### Mark Smucker, University of Waterloo
#### Guido Zuccon, University of Queensland


## Contact
For more information, contact trec.decision@gmail.com 
